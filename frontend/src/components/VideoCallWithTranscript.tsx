import React, { useEffect, useRef, useState, useCallback } from 'react';
import {
  Box,
  Paper,
  Typography,
  IconButton,
  List,
  ListItem,
  Chip,
  Alert,
  Button,
  Dialog,
  DialogTitle,
  DialogContent,
  DialogActions,
  Grid,
  ListItemText,
} from '@mui/material';
import {
  Mic as MicIcon,
  MicOff as MicOffIcon,
  Videocam as VideocamIcon,
  VideocamOff as VideocamOffIcon,
  CallEnd as CallEndIcon,
} from '@mui/icons-material';
import { useWebRTC } from '../hooks/useWebRTC';
import { useSharedSpeechRecognition } from '../hooks/useSharedSpeechRecognition';

interface VideoCallWithTranscriptProps {
  callId: string;
  onEndCall: () => void;
  isIncoming?: boolean;
  shouldStartCall?: boolean;
  showUI?: boolean;
}

interface TranscriptEntry {
  user_id: string;
  text: string;
  timestamp: string;
}

export const VideoCallWithTranscript: React.FC<VideoCallWithTranscriptProps> = ({
  callId,
  onEndCall,
  isIncoming = false,
  shouldStartCall = true,
  showUI = true,
}) => {
  const wsRef = useRef<WebSocket | null>(null);
  const [transcript, setTranscript] = useState<TranscriptEntry[]>([]);
  const [isCallActive, setIsCallActive] = useState(true);
  const [audioEnabled, setAudioEnabled] = useState(true);
  const [videoEnabled, setVideoEnabled] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [isWebSocketConnected, setIsWebSocketConnected] = useState(false);
  const transcriptRef = useRef<HTMLDivElement>(null);
  const isInitializedRef = useRef(false);
  
  // –†–µ—Ñ—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è speech recognition
  const speechStartedRef = useRef(false);
  const restartCountRef = useRef(0);
  const maxRestarts = isIncoming ? 3 : 5; // –ú–µ–Ω—å—à–µ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –≤—Ö–æ–¥—è—â–∏—Ö –∑–≤–æ–Ω–∫–æ–≤

  // –ö–æ–ª–ª–±–µ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
  const handleTranscript = useCallback((text: string) => {
    if (text.trim() && wsRef.current?.readyState === WebSocket.OPEN) {
      console.log('üìù Sending transcript:', text);
      
      const message = JSON.stringify({
        type: 'speech-transcript',
        text: text,
        timestamp: new Date().toISOString(),
      });
      
      wsRef.current.send(message);
    }
  }, []); // –ü—É—Å—Ç—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ - —Ñ—É–Ω–∫—Ü–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–∞

  // –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è WebRTC
  const {
    localStream,
    remoteStream,
    isConnected,
    startCall,
    endCall,
    toggleAudio,
    toggleVideo,
    handleSignal,
    getAudioStream,
  } = useWebRTC((signal) => {
    // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º WebRTC —Å–∏–≥–Ω–∞–ª—ã —á–µ—Ä–µ–∑ WebSocket
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send(JSON.stringify({
        type: 'webrtc-signal',
        signal: signal,
      }));
      console.log('üì§ Sent WebRTC signal:', signal.type);
    } else {
      console.error('‚ùå WebSocket not ready for signal:', signal.type);
    }
  }, isIncoming);

  // –•—É–∫ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏
  const {
    isListening,
    transcript: currentTranscript,
    startListening,
    stopListening,
  } = useSharedSpeechRecognition(handleTranscript);

  // –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ isListening –¥–ª—è —Å–±—Ä–æ—Å–∞ —Ñ–ª–∞–≥–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
  const prevIsListeningRef = useRef(isListening);
  useEffect(() => {
    const wasListening = prevIsListeningRef.current;
    const isCurrentlyListening = isListening;
    
    // –ï—Å–ª–∏ —Ä–µ—á–µ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–æ—Å—å (–±—ã–ª–æ –∞–∫—Ç–∏–≤–Ω–æ, —Å—Ç–∞–ª–æ –Ω–µ–∞–∫—Ç–∏–≤–Ω–æ)
    if (wasListening && !isCurrentlyListening) {
      console.log('üé§ Speech recognition stopped, resetting speechStartedRef');
      speechStartedRef.current = false;
      
      // –ü—Ä–æ—Å—Ç–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ –∑–∞–¥–µ—Ä–∂–∫—É
      if (isConnected && isInitializedRef.current && shouldStartCall) {
        if (restartCountRef.current < maxRestarts) {
          console.log(`üîÑ Auto-restart speech recognition (${restartCountRef.current + 1}/${maxRestarts})`);
          restartCountRef.current += 1;
          const restartDelay = isIncoming ? 3000 : 1500; // –£–º–µ—Ä–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏
          setTimeout(() => {
            // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º
            if (!speechStartedRef.current && !isListening && isConnected && isInitializedRef.current && shouldStartCall) {
              console.log('üîÑ Auto-restarting speech recognition...');
              speechStartedRef.current = true;
              const audioStream = getAudioStream();
              if (audioStream) {
                startListening(audioStream);
              } else {
                console.warn('‚ö†Ô∏è No audio stream for restart, resetting speechStarted');
                speechStartedRef.current = false;
              }
            } else {
              console.log('‚ö†Ô∏è Auto-restart conditions not met', {
                speechStarted: speechStartedRef.current,
                isListening,
                isConnected,
                isInitialized: isInitializedRef.current,
                shouldStartCall
              });
            }
          }, restartDelay);
        } else {
          console.log('‚ö†Ô∏è Max restart attempts reached for auto-restart');
        }
      }
    }
    
    prevIsListeningRef.current = isListening;
  }, [isListening, isConnected, shouldStartCall, getAudioStream, startListening, isIncoming]);

  // –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ WebSocket
  useEffect(() => {
    let websocket: WebSocket | null = null;
    let isMounted = true;
    let connectionAttempted = false; // –§–ª–∞–≥ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π

    const setupWebSocket = async () => {
      // –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
      if (connectionAttempted) {
        console.log('WebSocket connection already attempted, skipping...');
        return;
      }
      
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        console.log('WebSocket already connected, skipping...');
        setIsWebSocketConnected(true);
        return;
      }
      
      connectionAttempted = true;

      const token = localStorage.getItem('access_token');
      if (!token) {
        setError('–¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω');
        return;
      }

      if (!isMounted) return; // –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –µ—Å–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —É–∂–µ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω

      console.log('Setting up WebSocket connection for callId:', callId);
      
      // –ò—Å–ø–æ–ª—å–∑—É–µ–º localhost:8080 –¥–ª—è development, —Ç–∞–∫ –∫–∞–∫ –±—ç–∫–µ–Ω–¥ –∑–∞–ø—É—â–µ–Ω –Ω–∞ —ç—Ç–æ–º –ø–æ—Ä—Ç—É
      const websocketUrl = `ws://localhost:8080/api/v1/call/${callId}/ws?token=${token}`;

      websocket = new WebSocket(websocketUrl);

      websocket.onopen = () => {
        if (!isMounted) {
          websocket?.close();
          return;
        }
        console.log('WebSocket connected');
        wsRef.current = websocket;
        setIsWebSocketConnected(true);
        setError(null);
      };

      websocket.onmessage = async (event) => {
        if (!isMounted) return;
        
        try {
          const data = JSON.parse(event.data);
          console.log('Received WebSocket message:', data);

          switch (data.type) {
            case 'webrtc-signal':
              // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º WebRTC —Å–∏–≥–Ω–∞–ª—ã
              console.log('üì• Received WebRTC signal:', data.signal.type, data.signal);
              try {
                await handleSignal(data.signal);
                console.log('‚úÖ WebRTC signal processed successfully');
              } catch (error) {
                console.error('‚ùå Error processing WebRTC signal:', error);
              }
              break;

            case 'transcript':
              // –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
              setTranscript(prev => [...prev, {
                user_id: data.user_id,
                text: data.text,
                timestamp: data.timestamp,
              }]);
              break;

            case 'call-ended':
              // –ó–≤–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω
              if (isMounted) {
                setIsCallActive(false);
                onEndCall();
              }
              break;
          }
        } catch (error) {
          console.error('Error parsing WebSocket message:', error);
        }
      };

      websocket.onclose = (event) => {
        console.log('WebSocket disconnected', { code: event.code, reason: event.reason });
        if (isMounted) {
          wsRef.current = null;
          setIsWebSocketConnected(false);
          isInitializedRef.current = false; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
        }
      };

      websocket.onerror = (error) => {
        console.error('WebSocket error:', error);
        if (isMounted) {
          console.log('üîÑ WebSocket error detected, will retry connection...');
          // –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—à–∏–±–∫—É —Å—Ä–∞–∑—É - –¥–∞–¥–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è
        }
      };
    };

    setupWebSocket();

    return () => {
      console.log('Cleaning up WebSocket connection');
      isMounted = false;
      isInitializedRef.current = false;
      if (websocket) {
        if (websocket.readyState === WebSocket.OPEN || websocket.readyState === WebSocket.CONNECTING) {
          websocket.close();
        }
      }
    };
  }, [callId]); // –¢–æ–ª—å–∫–æ callId –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö

  // –ê–≤—Ç–æ—Å–∫—Ä–æ–ª–ª —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
  useEffect(() => {
    if (transcriptRef.current) {
      transcriptRef.current.scrollTop = transcriptRef.current.scrollHeight;
    }
  }, [transcript]);

  // –ù–∞—á–∏–Ω–∞–µ–º –∑–≤–æ–Ω–æ–∫ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
  useEffect(() => {
    const initializeCall = async () => {
      if (isInitializedRef.current) {
        return; // –£–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
      }

      try {
        console.log('Initializing call...', { 
          isWebSocketConnected, 
          isIncoming, 
          shouldStartCall,
          isInitialized: isInitializedRef.current 
        });
        
        if (isWebSocketConnected && shouldStartCall) {
          isInitializedRef.current = true;
          
          // –ó–∞–ø—É—Å–∫–∞–µ–º WebRTC –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –∑–≤–æ–Ω–∫–∞
          console.log('Starting WebRTC call...');
          await startCall();
          
          // –û–¢–ö–õ–ê–î–´–í–ê–ï–ú –∑–∞–ø—É—Å–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –¥–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
          console.log('WebRTC started, waiting for connection before speech recognition...');
        }
      } catch (error) {
        console.error('Error initializing call:', error);
        setError('–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–≤–æ–Ω–∫–∞');
        isInitializedRef.current = false; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥ –ø—Ä–∏ –æ—à–∏–±–∫–µ
      }
    };

    if (isWebSocketConnected && shouldStartCall && !isInitializedRef.current) {
      initializeCall();
    }
  }, [isWebSocketConnected, isIncoming, shouldStartCall]);

  // –û—Ç–¥–µ–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ä–µ—á–µ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è WebRTC —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
  useEffect(() => {
    if (isConnected && isInitializedRef.current && shouldStartCall && !speechStartedRef.current) {
      console.log('WebRTC connected! Starting speech recognition with delay...', {
        isConnected,
        isInitialized: isInitializedRef.current,
        shouldStartCall,
        speechStarted: speechStartedRef.current,
        isIncoming
      });
      speechStartedRef.current = true; // –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—É—Å–∫–∏
      
      // –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —á—Ç–æ–±—ã WebRTC –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª—Å—è
      const speechTimeout = setTimeout(() => {
        console.log('Attempting to start shared speech recognition...');
        try {
          const audioStream = getAudioStream();
          if (audioStream) {
            // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≤—Ö–æ–¥—è—â–∏—Ö –∑–≤–æ–Ω–∫–æ–≤
            if (isIncoming) {
              const audioTracks = audioStream.getAudioTracks();
              console.log('üîç Audio track validation for incoming call:', {
                tracksCount: audioTracks.length,
                enabled: audioTracks[0]?.enabled,
                readyState: audioTracks[0]?.readyState,
                muted: audioTracks[0]?.muted
              });
              
              // –ñ–¥–µ–º –µ—â–µ –Ω–µ–º–Ω–æ–≥–æ –µ—Å–ª–∏ —Ç—Ä–µ–∫ –Ω–µ –≥–æ—Ç–æ–≤
              if (audioTracks.length === 0 || audioTracks[0]?.readyState !== 'live') {
                console.warn('‚ö†Ô∏è Audio track not ready for incoming call, retrying...');
                speechStartedRef.current = false;
                setTimeout(() => {
                  const retryStream = getAudioStream();
                  if (retryStream) {
                    console.log('üîÑ Retry starting speech recognition for incoming call');
                    speechStartedRef.current = true;
                    startListening(retryStream);
                    restartCountRef.current = 0; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º retry
                  }
                }, 3000);
                return;
              }
            }
            
            console.log('‚úÖ Audio stream available, starting speech recognition');
            startListening(audioStream);
            console.log('‚úÖ Shared speech recognition started with WebRTC audio stream');
            restartCountRef.current = 0; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—É—Å–∫–µ
          } else {
            console.warn('‚ö†Ô∏è No audio stream available for speech recognition');
            speechStartedRef.current = false; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å
          }
        } catch (error) {
          console.error('Failed to start speech recognition:', error);
          speechStartedRef.current = false; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–µ
        }
      }, isIncoming ? 8000 : 3000); // –î–ª—è –≤—Ö–æ–¥—è—â–∏—Ö –∑–≤–æ–Ω–∫–æ–≤ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–æ 8 —Å–µ–∫—É–Ω–¥

      return () => {
        clearTimeout(speechTimeout);
      };
    } else {
      console.log('Speech recognition conditions not met:', {
        isConnected,
        isInitialized: isInitializedRef.current,
        shouldStartCall,
        speechStarted: speechStartedRef.current,
        isIncoming,
        isListening,
        hasAudioStream: !!getAudioStream()
      });
    }
  }, [isConnected, shouldStartCall, startListening, getAudioStream, isIncoming]);

  const handleEndCall = useCallback(() => {
    console.log('üìû Ending call...');
    
    // –°–Ω–∞—á–∞–ª–∞ –ø–æ–º–µ—á–∞–µ–º —á—Ç–æ –∑–≤–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è
    isInitializedRef.current = false;
    speechStartedRef.current = false; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥ —Ä–µ—á–µ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
    
    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
    try {
      stopListening();
      console.log('üé§ Speech recognition stopped');
    } catch (e) {
      console.error('Error stopping speech recognition:', e);
    }
    
    // –ó–∞–≤–µ—Ä—à–∞–µ–º WebRTC –∑–≤–æ–Ω–æ–∫
    try {
      endCall();
      console.log('üìû WebRTC call ended');
    } catch (e) {
      console.error('Error ending WebRTC call:', e);
    }
    
    // –ó–∞–∫—Ä—ã–≤–∞–µ–º WebSocket
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      console.log('üîå Closing WebSocket...');
      wsRef.current.close();
    }
    
    // –í—ã–∑—ã–≤–∞–µ–º –∫–æ–ª–ª–±–µ–∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
    onEndCall?.();
  }, [endCall, onEndCall, stopListening]);

  const handleToggleAudio = useCallback(() => {
    toggleAudio();
    setAudioEnabled(!audioEnabled);
  }, [toggleAudio, audioEnabled]);

  const handleToggleVideo = useCallback(() => {
    toggleVideo();
    setVideoEnabled(!videoEnabled);
  }, [toggleVideo, videoEnabled]);

  // –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Ä–µ—á–µ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
  const handleRestartSpeechRecognition = useCallback(() => {
    console.log('üîÑ Manual restart of speech recognition...');
    speechStartedRef.current = false;
    restartCountRef.current = 0; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø—Ä–∏ —Ä—É—á–Ω–æ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ
    
    setTimeout(() => {
      const audioStream = getAudioStream();
      if (audioStream && isConnected) {
        console.log('üîÑ Manual restart: starting speech recognition');
        speechStartedRef.current = true;
        startListening(audioStream);
      } else {
        console.warn('‚ö†Ô∏è Manual restart failed: no audio stream or not connected');
      }
    }, 500);
  }, [getAudioStream, isConnected, startListening]);

  // Cleanup –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
  useEffect(() => {
    return () => {
      console.log('üßπ Cleaning up VideoCallWithTranscript...');
      
      // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ —Ñ–ª–∞–≥–∏
      isInitializedRef.current = false;
      speechStartedRef.current = false;
      
      // –ó–∞–∫—Ä—ã–≤–∞–µ–º WebSocket –µ—Å–ª–∏ –æ–Ω –æ—Ç–∫—Ä—ã—Ç
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        console.log('üîå Closing WebSocket in cleanup...');
        wsRef.current.close();
      }
    };
  }, []); // –ü—É—Å—Ç—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

  if (error) {
    return (
      <Dialog open={true} onClose={onEndCall} maxWidth="sm" fullWidth>
        <DialogTitle>–û—à–∏–±–∫–∞ –≤–∏–¥–µ–æ–∑–≤–æ–Ω–∫–∞</DialogTitle>
        <DialogContent>
          <Alert severity="error">{error}</Alert>
        </DialogContent>
        <DialogActions>
          <Button onClick={onEndCall} variant="contained">
            –ó–∞–∫—Ä—ã—Ç—å
          </Button>
        </DialogActions>
      </Dialog>
    );
  }

  if (!isCallActive) {
    return (
      <Dialog open={true} onClose={onEndCall} maxWidth="sm" fullWidth>
        <DialogTitle>–ó–≤–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω</DialogTitle>
        <DialogContent>
          <Typography>–í–∏–¥–µ–æ–∑–≤–æ–Ω–æ–∫ –±—ã–ª –∑–∞–≤–µ—Ä—à–µ–Ω</Typography>
        </DialogContent>
        <DialogActions>
          <Button onClick={onEndCall} variant="contained">
            –ó–∞–∫—Ä—ã—Ç—å
          </Button>
        </DialogActions>
      </Dialog>
    );
  }

  if (!showUI) {
    return null;
  }

  return (
    <Box sx={{
      position: 'fixed',
      top: 0,
      left: 0,
      right: 0,
      bottom: 0,
      bgcolor: 'black',
      display: 'flex',
      zIndex: 9999,
    }}>
      {/* –í–∏–¥–µ–æ –æ–±–ª–∞—Å—Ç—å */}
      <Box sx={{ flex: 1, position: 'relative' }}>
        {/* –£–¥–∞–ª–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ */}
        <Box
          component="video"
          ref={(video: HTMLVideoElement) => {
            if (video && remoteStream) {
              video.srcObject = remoteStream;
            }
          }}
          autoPlay
          playsInline
          sx={{
            width: '100%',
            height: '100%',
            objectFit: 'cover',
            bgcolor: 'grey.900',
          }}
        />

        {/* –õ–æ–∫–∞–ª—å–Ω–æ–µ –≤–∏–¥–µ–æ (–∫–∞—Ä—Ç–∏–Ω–∫–∞ –≤ –∫–∞—Ä—Ç–∏–Ω–∫–µ) */}
        <Paper sx={{
          position: 'absolute',
          top: 2,
          right: 2,
          width: 192,
          height: 144,
          overflow: 'hidden',
          bgcolor: 'grey.800',
        }}>
          <Box
            component="video"
            ref={(video: HTMLVideoElement) => {
              if (video && localStream) {
                video.srcObject = localStream;
              }
            }}
            autoPlay
            playsInline
            muted
            sx={{
              width: '100%',
              height: '100%',
              objectFit: 'cover',
            }}
          />
        </Paper>

        {/* –°—Ç–∞—Ç—É—Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è */}
        <Box sx={{ position: 'absolute', top: 2, left: 2 }}>
          <Chip
            icon={<Box sx={{
              width: 8,
              height: 8,
              borderRadius: '50%',
              bgcolor: isConnected ? 'success.main' : 'error.main',
            }} />}
            label={isConnected ? '–ü–æ–¥–∫–ª—é—á–µ–Ω–æ' : '–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ...'}
            color={isConnected ? 'success' : 'error'}
            variant="filled"
          />
        </Box>

        {/* –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ */}
        {isListening && (
          <Box sx={{ position: 'absolute', bottom: 10, left: 2 }}>
            <Chip
              icon={<Box sx={{
                width: 8,
                height: 8,
                borderRadius: '50%',
                bgcolor: 'error.main',
                animation: 'pulse 1s infinite',
              }} />}
              label="–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∞–∫—Ç–∏–≤–Ω–æ"
              color="error"
              variant="filled"
            />
          </Box>
        )}

        {/* –¢–µ–∫—É—â–∏–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç */}
        {currentTranscript && (
          <Paper sx={{
            position: 'absolute',
            bottom: 16,
            left: 2,
            right: 2,
            p: 2,
            bgcolor: 'rgba(0, 0, 0, 0.7)',
            color: 'white',
          }}>
            <Typography variant="caption" sx={{ opacity: 0.8, display: 'block' }}>
              –í—ã –≥–æ–≤–æ—Ä–∏—Ç–µ:
            </Typography>
            <Typography variant="body2">{currentTranscript}</Typography>
          </Paper>
        )}

        {/* –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è */}
        <Box sx={{
          position: 'absolute',
          bottom: 2,
          left: '50%',
          transform: 'translateX(-50%)',
          display: 'flex',
          gap: 2,
        }}>
          <IconButton
            onClick={handleToggleAudio}
            sx={{
              bgcolor: audioEnabled ? 'grey.700' : 'error.main',
              color: 'white',
              '&:hover': {
                bgcolor: audioEnabled ? 'grey.600' : 'error.dark',
              },
            }}
          >
            {audioEnabled ? <MicIcon /> : <MicOffIcon />}
          </IconButton>
          <IconButton
            onClick={handleToggleVideo}
            sx={{
              bgcolor: videoEnabled ? 'grey.700' : 'error.main',
              color: 'white',
              '&:hover': {
                bgcolor: videoEnabled ? 'grey.600' : 'error.dark',
              },
            }}
          >
            {videoEnabled ? <VideocamIcon /> : <VideocamOffIcon />}
          </IconButton>
          <IconButton
            onClick={handleEndCall}
            sx={{
              bgcolor: 'error.main',
              color: 'white',
              '&:hover': {
                bgcolor: 'error.dark',
              },
            }}
          >
            <CallEndIcon />
          </IconButton>
        </Box>
      </Box>

      {/* –ü–∞–Ω–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ */}
      <Paper sx={{ width: 384, display: 'flex', flexDirection: 'column' }}>
        <Box sx={{ p: 2, borderBottom: 1, borderColor: 'divider' }}>
          <Typography variant="h6">–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞</Typography>
          {/* –î–æ–±–∞–≤–ª—è–µ–º –∫–Ω–æ–ø–∫—É –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ */}
          <Box sx={{ display: 'flex', alignItems: 'center', gap: 1, mt: 1 }}>
            <Chip
              size="small"
              label={isListening ? '–ê–∫—Ç–∏–≤–Ω–æ' : '–ù–µ –∞–∫—Ç–∏–≤–Ω–æ'}
              color={isListening ? 'success' : 'error'}
            />
            <Button
              size="small"
              variant="outlined"
              onClick={handleRestartSpeechRecognition}
              disabled={!isConnected}
            >
              –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
            </Button>
          </Box>
        </Box>

        <Box
          ref={transcriptRef}
          sx={{ flex: 1, p: 2, overflow: 'auto' }}
        >
          {transcript.length === 0 ? (
            <Box sx={{ textAlign: 'center', py: 4 }}>
              <Typography color="text.secondary">
                –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø–æ—è–≤–∏—Ç—Å—è –∑–¥–µ—Å—å –≤–æ –≤—Ä–µ–º—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
              </Typography>
            </Box>
          ) : (
            <List sx={{ p: 0 }}>
              {transcript.map((entry, index) => (
                <ListItem key={index} sx={{ px: 0, py: 1, flexDirection: 'column', alignItems: 'flex-start' }}>
                  <Box sx={{ display: 'flex', alignItems: 'center', mb: 0.5, width: '100%' }}>
                    <Typography variant="caption" color="text.secondary" sx={{ mr: 1 }}>
                      –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {entry.user_id.slice(0, 8)}
                    </Typography>
                    <Typography variant="caption" color="text.secondary">
                      {new Date(entry.timestamp).toLocaleTimeString()}
                    </Typography>
                  </Box>
                  <Paper variant="outlined" sx={{ p: 1.5, width: '100%', borderLeft: 3, borderLeftColor: 'primary.main' }}>
                    <Typography variant="body2">{entry.text}</Typography>
                  </Paper>
                </ListItem>
              ))}
            </List>
          )}
        </Box>
      </Paper>
    </Box>
  );
}; 