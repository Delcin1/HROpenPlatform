import { useEffect, useRef, useState } from 'react';

interface SharedSpeechRecognitionHook {
  isListening: boolean;
  transcript: string;
  startListening: (audioStream: MediaStream | null) => void;
  stopListening: () => void;
  isSupported: boolean;
}

export const useSharedSpeechRecognition = (
  onTranscript?: (text: string) => void
): SharedSpeechRecognitionHook => {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isSupported, setIsSupported] = useState(false);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const timeoutRef = useRef<number | null>(null);
  const currentStreamRef = useRef<MediaStream | null>(null);
  const isStartingRef = useRef<boolean>(false);
  const shouldBlockAutoRestartRef = useRef<boolean>(false); // –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∞–≤—Ç–æ-–ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞

  useEffect(() => {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É Speech Recognition API
    const SpeechRecognition = 
      window.SpeechRecognition || 
      (window as any).webkitSpeechRecognition;

    if (SpeechRecognition) {
      setIsSupported(true);
      
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'ru-RU';

      recognition.onstart = () => {
        console.log('üé§ Shared speech recognition started successfully');
        setIsListening(true);
        isStartingRef.current = false;
        shouldBlockAutoRestartRef.current = false; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º —Å—Ç–∞—Ä—Ç–µ
      };

      recognition.onresult = (event) => {
        let finalTranscript = '';
        let interimTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          if (result.isFinal) {
            finalTranscript += result[0].transcript;
          } else {
            interimTranscript += result[0].transcript;
          }
        }

        // –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
        setTranscript(interimTranscript || finalTranscript);

        // –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –µ–≥–æ
        if (finalTranscript && onTranscript) {
          console.log('üé§ Final transcript:', finalTranscript.trim());
          onTranscript(finalTranscript.trim());
          
          // –û—á–∏—â–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —á–µ—Ä–µ–∑ –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É
          if (timeoutRef.current) {
            clearTimeout(timeoutRef.current);
          }
          timeoutRef.current = window.setTimeout(() => {
            setTranscript('');
          }, 1000);
        }
      };

      recognition.onerror = (event) => {
        console.error('‚ùå Shared speech recognition error:', event.error);
        setIsListening(false);
        isStartingRef.current = false;
        
        // –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –±–µ–∑ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤
        switch (event.error) {
          case 'no-speech':
            console.log('ü§´ No speech detected - this is normal');
            // –¢–æ–ª—å–∫–æ –û–î–ò–ù –±—ã—Å—Ç—Ä—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –ø—Ä–∏ no-speech
            if (currentStreamRef.current && !isStartingRef.current) {
              setTimeout(() => {
                console.log('üîÑ Single restart after no-speech...');
                if (currentStreamRef.current && !isStartingRef.current && !isListening) {
                  startListening(currentStreamRef.current);
                }
              }, 1000); // –£–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–æ 1 —Å–µ–∫—É–Ω–¥—ã
            }
            break;
            
          case 'audio-capture':
            console.log('üé§ Audio capture error - likely microphone conflict');
            // –ù–ï –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ audio-capture
            console.log('‚ö†Ô∏è Speech recognition stopped due to audio conflict');
            currentStreamRef.current = null; // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∏
            break;
            
          case 'network':
            console.log('üåê Network error in speech recognition');
            // –ë–ª–æ–∫–∏—Ä—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∏–∑ onend –ø—Ä–∏ network –æ—à–∏–±–∫–∞—Ö
            shouldBlockAutoRestartRef.current = true;
            console.log('‚ö†Ô∏è Speech recognition paused due to network error - blocking auto-restart');
            break;
            
          case 'aborted':
            console.log('‚èπÔ∏è Speech recognition aborted');
            // –ù–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏ aborted - —ç—Ç–æ –æ–±—ã—á–Ω–æ –Ω–∞–º–µ—Ä–µ–Ω–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            break;
            
          case 'not-allowed':
            console.error('üö´ Microphone access not allowed');
            break;
            
          default:
            console.log(`‚ö†Ô∏è Speech recognition error: ${event.error}`);
            // –¢–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫ –ø—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
            if (currentStreamRef.current && !isStartingRef.current && event.error !== 'network' && event.error !== 'audio-capture') {
              setTimeout(() => {
                console.log('üîÑ Retrying after unknown error...');
                startListening(currentStreamRef.current);
              }, 5000);
            }
        }
      };

      recognition.onend = () => {
        console.log('üîö Shared speech recognition ended');
        setIsListening(false);
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –∞–≤—Ç–æ-–ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
        if (shouldBlockAutoRestartRef.current) {
          console.log('üö´ Auto-restart blocked due to previous error');
          shouldBlockAutoRestartRef.current = false; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥
          return;
        }
        
        // –ü—Ä–æ—Å—Ç–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ—Ç–æ–∫
        if (currentStreamRef.current && !isStartingRef.current) {
          console.log('üîÑ Auto-restarting speech recognition after natural end...');
          setTimeout(() => {
            if (currentStreamRef.current && !isStartingRef.current && !isListening) {
              startListening(currentStreamRef.current);
            }
          }, 2000); // 2 —Å–µ–∫—É–Ω–¥—ã –∑–∞–¥–µ—Ä–∂–∫–∞
        } else {
          console.log('‚ÑπÔ∏è Speech recognition ended - no auto-restart (no stream or starting)');
        }
      };

      recognitionRef.current = recognition;
    } else {
      console.warn('‚ö†Ô∏è Speech Recognition API not supported');
      setIsSupported(false);
    }

    return () => {
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
      }
    };
  }, [onTranscript]);

  const startListening = (audioStream: MediaStream | null) => {
    if (!audioStream) {
      console.warn('‚ö†Ô∏è No audio stream provided for speech recognition');
      return;
    }

    if (!recognitionRef.current || !isSupported) {
      console.warn('‚ö†Ô∏è Speech recognition not available');
      return;
    }

    if (isListening || isStartingRef.current) {
      console.log('üé§ Speech recognition already running or starting');
      return;
    }

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∞—É–¥–∏–æ —Ç—Ä–µ–∫ –∞–∫—Ç–∏–≤–µ–Ω
    const audioTracks = audioStream.getAudioTracks();
    if (audioTracks.length === 0 || !audioTracks[0].enabled) {
      console.warn('‚ö†Ô∏è No active audio track in stream');
      return;
    }

    try {
      console.log('üé§ Starting shared speech recognition...');
      console.log('üîä Audio stream info:', {
        tracks: audioTracks.length,
        enabled: audioTracks[0].enabled,
        readyState: audioTracks[0].readyState
      });
      
      // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ—Ç–æ–∫
      currentStreamRef.current = audioStream;
      isStartingRef.current = true;
      
      // –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
      // Speech Recognition API –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–π –º–∏–∫—Ä–æ—Ñ–æ–Ω
      recognitionRef.current.start();
      
    } catch (error) {
      console.error('‚ùå Failed to start shared speech recognition:', error);
      isStartingRef.current = false;
    }
  };

  const stopListening = () => {
    console.log('üõë Stopping shared speech recognition...');
    
    // –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ—Ç–æ–∫ —á—Ç–æ–±—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∏
    currentStreamRef.current = null;
    isStartingRef.current = false;
    
    if (recognitionRef.current && (isListening || isStartingRef.current)) {
      try {
        recognitionRef.current.stop();
      } catch (error) {
        console.warn('‚ö†Ô∏è Error stopping speech recognition:', error);
      }
    }
    
    setIsListening(false);
    setTranscript('');
  };

  return {
    isListening,
    transcript,
    startListening,
    stopListening,
    isSupported,
  };
}; 